{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "59fa684b11f75e8b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-26T17:00:28.098868Z",
     "start_time": "2024-09-26T16:59:46.663022Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device cuda\n",
      "tokens per iteration will be: 2,560\n",
      "Initializing a new model from scratch\n",
      "defaulting to vocab_size of GPT-2 to 50304 (50257 rounded up for efficiency)\n",
      "1.63M parameters (6.23MB)\n",
      "num decayed parameter tensors: 10, with 1,634,816 parameters\n",
      "num non-decayed parameter tensors: 5, with 160 parameters\n",
      "using fused AdamW: True\n",
      "step 0: train loss 10.8222, val loss 10.8219\n",
      "step 10: train loss 10.7763, val loss 10.7851\n",
      "saving checkpoint to results/tolstoy/ckpt_init.pt\n",
      "step 20: train loss 10.5780, val loss 10.5954\n",
      "saving checkpoint to results/tolstoy/ckpt.pt\n",
      "step 30: train loss 10.2757, val loss 10.2863\n",
      "saving checkpoint to results/tolstoy/ckpt.pt\n",
      "step 40: train loss 9.8405, val loss 9.8803\n",
      "saving checkpoint to results/tolstoy/ckpt.pt\n",
      "step 50: train loss 9.2679, val loss 9.2986\n",
      "saving checkpoint to results/tolstoy/ckpt.pt\n",
      "step 60: train loss 8.6045, val loss 8.6906\n",
      "saving checkpoint to results/tolstoy/ckpt.pt\n",
      "step 70: train loss 7.9481, val loss 8.0610\n",
      "saving checkpoint to results/tolstoy/ckpt.pt\n",
      "step 80: train loss 7.4131, val loss 7.4900\n",
      "saving checkpoint to results/tolstoy/ckpt.pt\n",
      "step 90: train loss 6.9273, val loss 7.1541\n",
      "saving checkpoint to results/tolstoy/ckpt.pt\n",
      "step 100: train loss 6.6185, val loss 6.7037\n",
      "saving checkpoint to results/tolstoy/ckpt.pt\n",
      "step 110: train loss 6.4535, val loss 6.5282\n",
      "saving checkpoint to results/tolstoy/ckpt.pt\n",
      "step 120: train loss 6.3481, val loss 6.5955\n",
      "step 130: train loss 6.5809, val loss 6.6121\n",
      "step 140: train loss 6.5593, val loss 6.6708\n",
      "step 150: train loss 6.2513, val loss 6.3931\n",
      "saving checkpoint to results/tolstoy/ckpt.pt\n",
      "step 160: train loss 6.1590, val loss 6.5915\n",
      "step 170: train loss 6.0841, val loss 6.4267\n",
      "step 180: train loss 6.0230, val loss 6.4951\n",
      "step 190: train loss 5.8694, val loss 6.1941\n",
      "saving checkpoint to results/tolstoy/ckpt.pt\n",
      "step 200: train loss 5.7798, val loss 5.9301\n",
      "saving checkpoint to results/tolstoy/ckpt.pt\n",
      "step 210: train loss 5.6295, val loss 5.9757\n",
      "step 220: train loss 5.4986, val loss 5.9100\n",
      "saving checkpoint to results/tolstoy/ckpt.pt\n",
      "step 230: train loss 5.6500, val loss 5.8375\n",
      "saving checkpoint to results/tolstoy/ckpt.pt\n",
      "step 240: train loss 5.3118, val loss 5.6907\n",
      "saving checkpoint to results/tolstoy/ckpt.pt\n",
      "step 250: train loss 5.2749, val loss 5.6326\n",
      "saving checkpoint to results/tolstoy/ckpt.pt\n",
      "step 260: train loss 5.3080, val loss 5.8287\n",
      "step 270: train loss 4.8895, val loss 5.1656\n",
      "saving checkpoint to results/tolstoy/ckpt.pt\n",
      "step 280: train loss 5.0618, val loss 5.4328\n",
      "step 290: train loss 5.1632, val loss 5.6302\n",
      "step 300: train loss 5.2205, val loss 5.2520\n",
      "step 310: train loss 5.0523, val loss 5.5531\n",
      "step 320: train loss 4.7677, val loss 5.5578\n",
      "step 330: train loss 4.8989, val loss 5.3251\n",
      "Validation loss did not improve for 5 iterations, stopping now as it is unlikely to get better.\n",
      "Training done, best_val_loss = 5.1655659675598145\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from llm.train import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model_path='results/tolstoy',  # output directory where the model will be saved\n",
    "    training_data_path='https://www.gutenberg.org/cache/epub/2600/pg2600.txt',  # dataset URL or local path\n",
    "    eval_interval=10,  # when to evaluate the model\n",
    "    batch_size=4,  # batch size\n",
    "    block_size=16,  # block size (aka context length)\n",
    "    n_layer=2,  # number of layers\n",
    "    n_head=4,  # number of attention heads per layer\n",
    "    n_embd=32,  # embedding dimension\n",
    "    dropout=0.2,  # dropout rate\n",
    "    learning_rate=0.05,  # learning rate\n",
    "    min_lr=0.005,  # minimum learning rate\n",
    "    beta2=0.99,  # adam beta2 (should be reduced for larger models / datasets)\n",
    ")\n",
    "trainer.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c165b710e4f46351",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-26T17:00:28.121230Z",
     "start_time": "2024-09-26T17:00:28.104185Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class Trainer in module llm.train:\n",
      "\n",
      "class Trainer(llm.ml.ML)\n",
      " |  Trainer(training_data_path: Optional[str] = MISSING, init_from: str = MISSING, torch_compile: bool = MISSING, train_ratio: float = MISSING, log_interval: int = MISSING, eval_interval: int = MISSING, eval_iters: int = MISSING, eval_only: bool = MISSING, always_save_checkpoint: bool = MISSING, override_checkpoint: bool = MISSING, wandb_log: bool = MISSING, wandb_project: str = MISSING, wandb_run_name: str = MISSING, encoding: str = MISSING, dataset: Optional[str] = MISSING, gradient_accumulation_steps: int = MISSING, batch_size: int = MISSING, block_size: int = MISSING, n_layer: int = MISSING, n_head: int = MISSING, n_embd: int = MISSING, dropout: float = MISSING, bias: bool = MISSING, learning_rate: float = MISSING, max_iters: int = MISSING, weight_decay: float = MISSING, beta1: float = MISSING, beta2: float = MISSING, grad_clip: float = MISSING, decay_lr: bool = MISSING, warmup_iters: int = MISSING, lr_decay_iters: int = MISSING, min_lr: float = MISSING, backend: str = MISSING, patience: int = MISSING, profile: bool = MISSING, profile_dir: str = MISSING, synchronize: bool = MISSING, **kwargs)\n",
      " |  \n",
      " |  This trainer can be run both on a single gpu in debug mode,\n",
      " |  and also in a larger training run with distributed data parallel (ddp).\n",
      " |  \n",
      " |  To run on a single GPU, example:\n",
      " |  $ python train.py\n",
      " |  \n",
      " |  To run with DDP on 4 gpus on 1 node, example:\n",
      " |  $ torchrun --standalone --nproc_per_node=4 train.py\n",
      " |  \n",
      " |  To run with DDP on 4 gpus across 2 nodes, example:\n",
      " |  - Run on the first (master) node with example IP 123.456.123.456:\n",
      " |  $ torchrun --nproc_per_node=8 --nnodes=2 --node_rank=0 --master_addr=123.456.123.456 --master_port=1234 train.py\n",
      " |  - Run on the worker node:\n",
      " |  $ torchrun --nproc_per_node=8 --nnodes=2 --node_rank=1 --master_addr=123.456.123.456 --master_port=1234 train.py\n",
      " |  (If your cluster does not have Infiniband interconnect prepend NCCL_IB_DISABLE=1)\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      Trainer\n",
      " |      llm.ml.ML\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, training_data_path: Optional[str] = MISSING, init_from: str = MISSING, torch_compile: bool = MISSING, train_ratio: float = MISSING, log_interval: int = MISSING, eval_interval: int = MISSING, eval_iters: int = MISSING, eval_only: bool = MISSING, always_save_checkpoint: bool = MISSING, override_checkpoint: bool = MISSING, wandb_log: bool = MISSING, wandb_project: str = MISSING, wandb_run_name: str = MISSING, encoding: str = MISSING, dataset: Optional[str] = MISSING, gradient_accumulation_steps: int = MISSING, batch_size: int = MISSING, block_size: int = MISSING, n_layer: int = MISSING, n_head: int = MISSING, n_embd: int = MISSING, dropout: float = MISSING, bias: bool = MISSING, learning_rate: float = MISSING, max_iters: int = MISSING, weight_decay: float = MISSING, beta1: float = MISSING, beta2: float = MISSING, grad_clip: float = MISSING, decay_lr: bool = MISSING, warmup_iters: int = MISSING, lr_decay_iters: int = MISSING, min_lr: float = MISSING, backend: str = MISSING, patience: int = MISSING, profile: bool = MISSING, profile_dir: str = MISSING, synchronize: bool = MISSING, **kwargs)\n",
      " |      :param training_data_path: path to data or key in data_paths.json\n",
      " |      :param dataset: dataset name  (used only for the directory name where to store the data)\n",
      " |      :param model_path: output directory, ignored if init_from is not 'resume'\n",
      " |      :param init_from: either 'resume' (from an model_path), 'scratch', or a gpt2 variant (e.g. 'gpt2-xl')\n",
      " |      :param torch_compile: use PyTorch 2.0 to compile the model to be faster\n",
      " |      :param device: 'cpu', 'cuda', 'cuda:0', 'cuda:1' etc., or try 'mps' on macbooks\n",
      " |      :param dtype: 'float16', 'bfloat16', 'float32', 'float64' etc.\n",
      " |      :param train_ratio: what fraction of the data to use for training\n",
      " |      :param eval_interval: how often to evaluate the model\n",
      " |      :param log_interval: how often to log loss info\n",
      " |      :param eval_iters: how many iters to evaluate\n",
      " |      :param eval_only: if True, run eval only\n",
      " |      :param always_save_checkpoint: if True, save checkpoints after each eval\n",
      " |      :param override_checkpoint: if True, override the previous checkpoint\n",
      " |      :param wandb_log: if True, log with wandb\n",
      " |      :param wandb_project: wandb project name\n",
      " |      :param wandb_run_name: wandb run name\n",
      " |      :param gradient_accumulation_steps: gradient accumulation steps, used to simulate larger batch sizes\n",
      " |      :param batch_size: batch size. if gradient_accumulation_steps > 1, this is the micro-batch size\n",
      " |      :param block_size: block size\n",
      " |      :param n_layer: number of layers\n",
      " |      :param n_head: number of heads\n",
      " |      :param n_embd: embedding dimension\n",
      " |      :param dropout: dropout. for pretraining 0 is good, for fine-tuning try 0.1+\n",
      " |      :param bias: bias\n",
      " |      :param learning_rate: learning rate\n",
      " |      :param max_iters: maximum number of iterations\n",
      " |      :param weight_decay: weight decay\n",
      " |      :param beta1: beta1 for adam\n",
      " |      :param beta2: beta2 for adam\n",
      " |      :param grad_clip: gradient clipping. disable if == 0.0\n",
      " |      :param decay_lr: whether to decay learning rate\n",
      " |      :param warmup_iters: warmup iterations\n",
      " |      :param lr_decay_iters: learning rate decay iterations. should be ~= max_iters per Chinchilla\n",
      " |      :param min_lr: minimum learning rate. should be ~= learning_rate/10 per Chinchilla\n",
      " |      :param backend: DDP backend. 'nccl', 'gloo', etc.\n",
      " |      :param patience: how many unimproved evals to wait before early stopping\n",
      " |      :param profile: if True, use torch profiler\n",
      " |      :param profile_dir: directory to store the profiles\n",
      " |      :param synchronize: host-device synchronization, for proper benchmarking of compute time per step\n",
      " |  \n",
      " |  estimate_loss(self)\n",
      " |      Helps estimate an arbitrarily accurate loss over either split using many batches\n",
      " |  \n",
      " |  resume(self)\n",
      " |  \n",
      " |  run(self)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from llm.ml.ML:\n",
      " |  \n",
      " |  compile_model(self)\n",
      " |  \n",
      " |  get_config(self)\n",
      " |      Return a dict of the attributes of this class\n",
      " |  \n",
      " |  manual_seed(self)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from llm.ml.ML:\n",
      " |  \n",
      " |  device_type\n",
      " |  \n",
      " |  is_cuda\n",
      " |  \n",
      " |  model_name\n",
      " |  \n",
      " |  seed\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from llm.ml.ML:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(Trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e32d77bff8db9f5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-26T17:00:28.634533Z",
     "start_time": "2024-09-26T17:00:28.210479Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device cuda\n",
      "Using model in results/tolstoy/ckpt.pt\n",
      "1.63M parameters (6.23MB)\n",
      "He decided to the battle beyond the door.\n",
      "the\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "d, the whole longer,� said he had the study, her been\n",
      "out than but he was Dósov.\n",
      "We the life... he did had see already should an coach-who and atsha!” man, “I said she as\n",
      "\n",
      "but\n",
      "in be seen a little heard a arm\n",
      "old up.\n",
      "After. The\n"
     ]
    }
   ],
   "source": [
    "from llm.sample import Sampler\n",
    "\n",
    "sampler = Sampler(\n",
    "    model_path='results/tolstoy',  # output directory where the model has been saved\n",
    ")\n",
    "generated_text = sampler.generate_text(\n",
    "    prompt='He decided to',  # prompt\n",
    "    max_tokens=100,  # number of tokens to generate\n",
    ")\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "af1ae1cca128d865",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-26T17:00:28.653700Z",
     "start_time": "2024-09-26T17:00:28.639549Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function __init__ in module llm.sample:\n",
      "\n",
      "__init__(self, checkpoint_name: str = 'last', init_from: str = 'resume', **kwargs)\n",
      "    :param checkpoint_name: name of the checkpoint to load, ignored if init_from is not 'resume'\n",
      "    :param init_from: either 'resume' (from a local model_path) or 'online' (from HuggingFace hub)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(Sampler.__init__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e533d5b1559bc40",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-26T17:00:28.704930Z",
     "start_time": "2024-09-26T17:00:28.690202Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function generate_text in module llm.sample:\n",
      "\n",
      "generate_text(self, prompt: str = '\\n', num_samples: int = 1, max_tokens: int = 100, temperature: float = 1.0, top_k: int = 200)\n",
      "    :param prompt: prompt to start generation.\n",
      "      Can be \"<|endoftext|>\" or etc. Can also specify a file, use as: \"FILE:prompt.txt\"\n",
      "    :param num_samples: number of samples to draw\n",
      "    :param max_tokens: number of tokens generated in each sample\n",
      "    :param temperature: 1.0 = no change, < 1.0 = less random, > 1.0 = more random, in predictions\n",
      "    :param top_k: retain only the top_k most likely tokens, clamp others to have 0 probability\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(Sampler.generate_text) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e758982f7f844b5b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-26T17:00:36.707099Z",
     "start_time": "2024-09-26T17:00:28.739084Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device cuda\n",
      "loading weights from pretrained gpt: gpt2\n",
      "forcing vocab_size=50257, block_size=1024, bias=True\n",
      "overriding dropout rate to 0.0\n",
      "123.65M parameters (471.70MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7faf9f50ad934bc9ab3d12447e3ef5b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b64c0cc924b74a87a8e90ce6ac2d7dfd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10561b57c5b641f58ece41e6f6db5652",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No meta.pkl found, assuming GPT-2 encodings\n",
      "Today I decided to write about some of the films I have seen. I began reading all the reviews and viewed some of them and watched the answers to some of the questions!\n",
      "\n",
      "Sean McDermott: Could you tell us a bit about what you observe about the genre coming from Sony Pictures?\n",
      "\n",
      "Jean-Pierre Dumont: Right now the genre is big. When it is larger it is different enough that they actually support a lot of what the big studios were pushing with this thing. For instance when Star Wars\n"
     ]
    }
   ],
   "source": [
    "sampler = Sampler(init_from='online', model_path='gpt2')\n",
    "print(sampler.generate_text(prompt='Today I decided to'))"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-26T18:02:15.298877Z",
     "start_time": "2024-09-26T18:01:58.136603Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from llm.interface import UserInterface\n",
    "\n",
    "ui = UserInterface(model_path='gpt2', model_kw=dict(init_from='online'))\n",
    "ui.run()"
   ],
   "id": "37584cf279eb65f5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device cpu\n",
      "loading weights from pretrained gpt: gpt2\n",
      "forcing vocab_size=50257, block_size=1024, bias=True\n",
      "overriding dropout rate to 0.0\n",
      "123.65M parameters (471.70MB)\n",
      "No meta.pkl found, assuming GPT-2 encodings\n"
     ]
    }
   ],
   "execution_count": 8
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
