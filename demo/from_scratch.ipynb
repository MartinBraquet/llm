{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llm.sample import Sampler\n",
    "from llm.train import Trainer\n",
    "\n",
    "config_file = 'train_prince.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overriding: init_from = scratch\n",
      "Overriding: model_path = prince-medium\n",
      "Overriding: dataset = prince\n",
      "Overriding: always_save_checkpoint = True\n",
      "Overriding: log_interval = 10\n",
      "Overriding: eval_interval = 100\n",
      "Overriding: eval_iters = 50\n",
      "Overriding: batch_size = 4\n",
      "Overriding: block_size = 256\n",
      "Overriding: n_layer = 6\n",
      "Overriding: n_head = 6\n",
      "Overriding: n_embd = 384\n",
      "Overriding: dropout = 0.2\n",
      "Overriding: learning_rate = 0.001\n",
      "Overriding: min_lr = 0.0001\n",
      "Overriding: max_iters = 500\n",
      "Overriding: lr_decay_iters = 500\n",
      "Overriding: beta2 = 0.99\n",
      "Overriding: warmup_iters = 100\n",
      "Using device cuda\n",
      "tokens per iteration will be: 40,960\n",
      "Initializing a new model from scratch\n",
      "defaulting to vocab_size of GPT-2 to 50304 (50257 rounded up for efficiency)\n",
      "29.94M parameters (114.21MB)\n",
      "num decayed parameter tensors: 26, with 30,031,872 parameters\n",
      "num non-decayed parameter tensors: 13, with 4,992 parameters\n",
      "using fused AdamW: True\n",
      "compiling the model...\n",
      "step 0: train loss 10.8893, val loss 10.8850\n",
      "step 10: train loss 9.6116, val loss 9.6735\n",
      "saving checkpoint to /workspace/llm/results/prince-medium/ckpt_init.pt\n",
      "iter 10: loss 9.6201, time 1110.54ms\n",
      "iter 20: loss 8.5688, time 301.55ms\n",
      "iter 30: loss 7.2817, time 491.48ms\n",
      "iter 40: loss 5.7104, time 305.51ms\n",
      "iter 50: loss 4.7319, time 304.27ms\n",
      "iter 60: loss 4.0828, time 311.26ms\n",
      "iter 70: loss 3.9300, time 312.82ms\n",
      "iter 80: loss 3.5264, time 313.80ms\n",
      "iter 90: loss 3.4454, time 474.42ms\n",
      "step 100: train loss 2.8073, val loss 4.6998\n",
      "saving checkpoint to /workspace/llm/results/prince-medium/ckpt.pt\n",
      "iter 110: loss 2.8765, time 274.20ms\n",
      "iter 120: loss 2.6179, time 494.27ms\n",
      "iter 130: loss 2.2156, time 414.12ms\n",
      "iter 140: loss 1.8461, time 417.48ms\n",
      "iter 150: loss 1.7608, time 412.44ms\n",
      "iter 160: loss 1.4333, time 412.47ms\n",
      "iter 170: loss 1.2855, time 478.95ms\n",
      "iter 180: loss 1.0533, time 477.60ms\n",
      "iter 190: loss 0.7072, time 418.53ms\n",
      "step 200: train loss 0.3555, val loss 6.4678\n",
      "saving checkpoint to /workspace/llm/results/prince-medium/ckpt.pt\n",
      "iter 210: loss 0.5520, time 269.21ms\n",
      "iter 220: loss 0.4813, time 268.25ms\n",
      "iter 230: loss 0.4416, time 304.24ms\n",
      "iter 240: loss 0.4615, time 268.80ms\n",
      "iter 250: loss 0.2614, time 496.13ms\n",
      "iter 260: loss 0.2803, time 477.42ms\n",
      "iter 270: loss 0.2099, time 379.31ms\n",
      "iter 280: loss 0.2162, time 499.94ms\n",
      "iter 290: loss 0.1954, time 494.49ms\n",
      "step 300: train loss 0.0793, val loss 7.1774\n",
      "saving checkpoint to /workspace/llm/results/prince-medium/ckpt.pt\n",
      "iter 310: loss 0.1446, time 503.48ms\n",
      "iter 320: loss 0.1409, time 501.32ms\n",
      "iter 330: loss 0.1627, time 502.56ms\n",
      "iter 340: loss 0.1536, time 503.99ms\n",
      "iter 350: loss 0.1276, time 500.46ms\n",
      "iter 360: loss 0.1188, time 500.91ms\n",
      "iter 370: loss 0.1478, time 502.53ms\n",
      "iter 380: loss 0.0989, time 504.39ms\n",
      "iter 390: loss 0.1066, time 500.19ms\n",
      "step 400: train loss 0.0495, val loss 7.5828\n",
      "saving checkpoint to /workspace/llm/results/prince-medium/ckpt.pt\n",
      "iter 410: loss 0.1333, time 502.45ms\n",
      "iter 420: loss 0.1209, time 501.90ms\n",
      "iter 430: loss 0.0957, time 385.17ms\n",
      "iter 440: loss 0.1136, time 380.62ms\n",
      "iter 450: loss 0.1095, time 496.76ms\n",
      "iter 460: loss 0.0774, time 501.60ms\n",
      "iter 470: loss 0.0804, time 498.45ms\n",
      "iter 480: loss 0.1062, time 498.18ms\n",
      "iter 490: loss 0.1116, time 379.62ms\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(config_file=config_file)\n",
    "trainer.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device cuda\n",
      "Using model in /workspace/llm/results/prince-medium/ckpt.pt\n",
      "29.94M parameters (114.21MB)\n",
      "Output:\n",
      "And now here is my secret, a very simple secret: It is only with the \n",
      "heart that one can see rightly; what is essential is invisible to the eye.” \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "“What is essential is invisible to the eye,” the little prince repeated, so that he would be sure to \n",
      "remember. \n",
      "\n",
      "“It is the time you have wasted for your rose that makes your rose so important.” \n",
      "\n",
      "“It is the time I have wasted for my rose...” said the little prince, so that he would be sure to \n",
      "remember. \n",
      "\n",
      "“Men have forgotten this truth,” said the fox. “But you must not forget it. You become \n",
      "responsible, forever, for what you have tamed. You are responsible for your rose...” \n",
      "\n",
      "“I am responsible for my rose,” the little prince repeated, so that he would be sure to remember.\n"
     ]
    }
   ],
   "source": [
    "sampler = Sampler(model_path=trainer.model_path)\n",
    "print('Output:')\n",
    "print(sampler.generate_text(prompt=\"And now here is my secret\", max_tokens=200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarize The Little Prince appears to be dreaming...” \n",
      "\n",
      "And yet the little prince sitting on top of a wall, with his feet, with his feet, with his feet, with his feet \n",
      "arrested. The little prince was not reply. “I saw nothing more. Make me another planet?” \n",
      "\n",
      "“I ask me tell you him,” the king hastened to assure him. “Sire, over what do you \n",
      "rule?�\n"
     ]
    }
   ],
   "source": [
    "print(sampler.generate_text(prompt=\"Summarize The Little Prince\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
