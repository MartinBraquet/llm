{
  "init_from": "gpt2-xl",
  "out_dir": "gpt2_finetuned",
  "dataset": "prince",
  "eval_interval": 20,
  "eval_iters": 10,
  "log_interval": 2,
  "always_save_checkpoint": true,
  "gradient_accumulation_steps": 8,
  "batch_size": 4,
  "dropout": 0.2,
  "learning_rate": 3e-4,
  "max_iters": 20,
  "lr_decay_iters": 20,
  "min_lr": 3e-5,
  "beta2": 0.99,
  "warmup_iters": 100
}
